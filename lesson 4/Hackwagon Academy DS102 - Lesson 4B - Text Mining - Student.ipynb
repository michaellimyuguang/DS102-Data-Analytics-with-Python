{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://i2.wp.com/hackwagon.com/wp-content/uploads/2017/02/Logo-Web-Export.png?ssl=1\" width=200/></center>\n",
    "<h1> Hackwagon Academy DS102 Lesson 4B </h1>\n",
    "<h2> Text Mining</h2> \n",
    "<h3> Lesson Outline </h3>\n",
    "\n",
    "- 1. [Text Analysis](#1)\n",
    "- 2. [Terminologies](#2)\n",
    "- 3. [Text Normalisation](#3)\n",
    "- 4. [Simple Cleaning](#4)\n",
    "    - 4.1 [Lowercase](#4.1)\n",
    "    - 4.2 [Strip Spaces](#4.2)\n",
    "    - 4.3 [Regex Cleaning](#4.3)\n",
    "    - [Practice I](#P1)\n",
    "- 5. [Tokenisation](#5)\n",
    "- 6. [Method 1 - Lemmatisation](#6)\n",
    "    - 6.1 [POS Tagging](#6.1)\n",
    "    - 6.2 [Lemmatisation](#6.2)\n",
    "- 7. [Method 2 - Stemming](#7)\n",
    "- 8. [Stemming vs. Lemming](#8)\n",
    "- 9. [Stop Word Removal](#9)\n",
    "- [Practice II](#P2)\n",
    "- 10. [Sentiment Analysis](#10)\n",
    "    - 10.1 [VADER](#10.1)\n",
    "    - 10.2 [Naive Bayes Classification](#10.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "<a id='1'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;1.</font><font color=\"salmon\"> Text Analysis </font> </h2></a>\n",
    "\n",
    "<img src=\"https://i.imgur.com/pqrH1r3.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "NLP is Natural Language Processing, finding useful insights from unstructured textual data. \n",
    "\n",
    "**Practical**\n",
    "\n",
    "* Spam Detection\n",
    "* Censorship / Filtering Sexually Explicit Content\n",
    "* Organizing / Categorizing Documents\n",
    "* Determining people's impression of your company\n",
    "\n",
    "**Theoretical**\n",
    "\n",
    "* Understanding intent of words (question answering)\n",
    "* Understanding references in sentences (coreference resolution)\n",
    "    * The man tried to pick up his son but he was weak. Who does \"he\" refer to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;2.</font><font color=\"salmon\"> Terminologies </font> </h2></a>\n",
    "\n",
    "In Text Mining, we often use the following terms to refer to collections of words\n",
    "\n",
    "- <b>Corpus</b>\n",
    "    - Large collection of texts, represented by documents\n",
    "    - Corpora is a collection of corpus\n",
    "- <b>Document</b>\n",
    "    - Contains multiple words and strung together therefore producing meaning\n",
    "- <b>Term</b>\n",
    "    - A word is a term\n",
    "\n",
    "<img src=\"https://i.imgur.com/F3fSS1v.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Corpus\n",
    "\n",
    "To do either Lemmatisation (Lemming) or Stemming, we will use the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Downloading word corpus\n",
    "nltk.download('punkt') # STEMMING\n",
    "nltk.download('averaged_perceptron_tagger') # < POS TAGGING\n",
    "nltk.download('wordnet') # LEMMATISATION\n",
    "nltk.download('stopwords') # STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;3.</font><font color=\"salmon\"> Text Normalisation </font> </h2></a>\n",
    "\n",
    "The purpose of text normalisation is to find/retain the <b>root form of a word.</b> Take the following as an example: \n",
    "\n",
    "<img src=\"https://i.imgur.com/kSWH6i7.png\" width=600 />\n",
    "\n",
    "Text Normalisation involves several steps and depending on your use case, you can pick either Stemming or Lemmatization.\n",
    "\n",
    "<img src=\"https://i.imgur.com/NuOTXxL.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;4.</font><font color=\"salmon\"> Simple Cleaning </font> </h2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.1'><h3>4.1 Lowercase</h3></a>\n",
    "\n",
    "Make all characters within a string a lower case with `.lower()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hackwagon'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"HACKWAGON\"\n",
    "name.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.2'><h3>4.2 Strip Spaces</h3></a>\n",
    "\n",
    "Remove extra spaces within the string by using the `.strip()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HACKWAGON'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"      HACKWAGON     \"\n",
    "name.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.3'><h3>4.3 Regex Cleaning</h3></a>\n",
    "\n",
    "Use the regular expression library to remove unwanted characters. To learn more about the regular expression library, click [here](https://regexone.com/)\n",
    "\n",
    "<img src=\"https://i.imgur.com/bJJ9MBD.png\" width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hackwagon'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "name = \"(Hackwagon)...&-$\"\n",
    "name = re.sub(\"[.®'&$’\\\"\\-()]\", \"\", name) #removes special characters\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='P1'><h2> <img src=\"https://cdn.shopify.com/s/files/1/1200/7374/products/book_aec28e76-52ec-44ab-bc01-41df1279c89f_550x825.png?v=1473897430\" width=25 align=\"left\"> <font color=\"darkorange\"> &nbsp; Practice I </font><font color=\"skyblue\"> * </font></h2></a>\n",
    "\n",
    "### Songs-100.csv\n",
    "\n",
    "Using the `songs-100.csv`, create a DataFrame called `songs_df` and preview the DataFrame with `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shape of You</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Despacito - Remix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Despacito (Featuring Daddy Yankee)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Something Just Like This</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm the One</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name\n",
       "0                        Shape of You\n",
       "1                   Despacito - Remix\n",
       "2  Despacito (Featuring Daddy Yankee)\n",
       "3            Something Just Like This\n",
       "4                         I'm the One"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "songs_df = pd.read_csv(\"songs-100.csv\")\n",
    "songs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do simple cleaning function \n",
    "\n",
    "Using what you've learnt earlier, create a function called `clean_names()` which will clean every song name by:\n",
    "\n",
    "* Removes characters with the pattern `[.®'&$’\\\"\\-()]`\n",
    "* Lowercases the string\n",
    "* Strips extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_names(song_title):\n",
    "    song_title = song_title.strip()\n",
    "    song_title = song_title.lower()\n",
    "    song_title = re.sub(\"[.®'&$’\\\"\\-()]\", \"\", song_title)\n",
    "    return song_title\n",
    "    # return the title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply cleaning function \n",
    "\n",
    "Apply the clean function on the `names` column and <b>reassign it back to the <code>names</code> column</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shape of You</td>\n",
       "      <td>shape of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Despacito - Remix</td>\n",
       "      <td>despacito  remix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Despacito (Featuring Daddy Yankee)</td>\n",
       "      <td>despacito featuring daddy yankee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Something Just Like This</td>\n",
       "      <td>something just like this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm the One</td>\n",
       "      <td>im the one</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name                     cleaned_title\n",
       "0                        Shape of You                      shape of you\n",
       "1                   Despacito - Remix                  despacito  remix\n",
       "2  Despacito (Featuring Daddy Yankee)  despacito featuring daddy yankee\n",
       "3            Something Just Like This          something just like this\n",
       "4                         I'm the One                        im the one"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_df['cleaned_title'] = songs_df['name'].apply(clean_names)\n",
    "songs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> End of Practice I </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;5.</font><font color=\"salmon\"> Tokenisation </font> </h2></a>\n",
    "\n",
    "Tokenisation is the process of splitting up each word by spaces into individual tokens. \n",
    "\n",
    "<img src=\"https://i.imgur.com/LSpV9Y1.png\" width=400>\n",
    "\n",
    "To do so, we will use the `nltk` - `word-tokenize()` function to tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['shape', 'of', 'you'],\n",
       " ['despacito', 'remix'],\n",
       " ['despacito', 'featuring', 'daddy', 'yankee'],\n",
       " ['something', 'just', 'like', 'this'],\n",
       " ['im', 'the', 'one'],\n",
       " ['humble'],\n",
       " ['it', 'aint', 'me', 'with', 'selena', 'gomez'],\n",
       " ['unforgettable'],\n",
       " ['thats', 'what', 'i', 'like'],\n",
       " ['i',\n",
       "  'dont',\n",
       "  'wan',\n",
       "  'na',\n",
       "  'live',\n",
       "  'forever',\n",
       "  'fifty',\n",
       "  'shades',\n",
       "  'darker',\n",
       "  'from',\n",
       "  'fifty',\n",
       "  'shades',\n",
       "  'darker',\n",
       "  'original',\n",
       "  'motion',\n",
       "  'picture',\n",
       "  'soundtrack'],\n",
       " ['xo', 'tour', 'llif3'],\n",
       " ['paris'],\n",
       " ['stay', 'with', 'alessia', 'cara'],\n",
       " ['attention'],\n",
       " ['mask', 'off'],\n",
       " ['congratulations'],\n",
       " ['swalla', 'feat', 'nicki', 'minaj', 'ty', 'dolla', 'ign'],\n",
       " ['castle', 'on', 'the', 'hill'],\n",
       " ['rockabye', 'feat', 'sean', 'paul', 'annemarie'],\n",
       " ['believer'],\n",
       " ['mi', 'gente'],\n",
       " ['thunder'],\n",
       " ['say', 'you', 'wont', 'let', 'go'],\n",
       " ['theres', 'nothing', 'holdin', 'me', 'back'],\n",
       " ['me', 'rehúso'],\n",
       " ['issues'],\n",
       " ['galway', 'girl'],\n",
       " ['scared', 'to', 'be', 'lonely'],\n",
       " ['closer'],\n",
       " ['symphony', 'feat', 'zara', 'larsson'],\n",
       " ['i', 'feel', 'it', 'coming'],\n",
       " ['starboy'],\n",
       " ['wild', 'thoughts'],\n",
       " ['slide'],\n",
       " ['new', 'rules'],\n",
       " ['18002738255'],\n",
       " ['passionfruit'],\n",
       " ['rockstar'],\n",
       " ['strip', 'that', 'down'],\n",
       " ['2u', 'feat', 'justin', 'bieber'],\n",
       " ['perfect'],\n",
       " ['call', 'on', 'me', 'ryan', 'riback', 'extended', 'remix'],\n",
       " ['feels'],\n",
       " ['mama'],\n",
       " ['felices', 'los', '4'],\n",
       " ['ispy', 'feat', 'lil', 'yachty'],\n",
       " ['location'],\n",
       " ['chantaje'],\n",
       " ['bad', 'and', 'boujee', 'feat', 'lil', 'uzi', 'vert'],\n",
       " ['havana'],\n",
       " ['solo', 'dance'],\n",
       " ['fake', 'love'],\n",
       " ['let', 'me', 'love', 'you'],\n",
       " ['more', 'than', 'you', 'know'],\n",
       " ['one', 'dance'],\n",
       " ['subeme', 'la', 'radio'],\n",
       " ['pretty', 'girl', 'cheat', 'codes', 'x', 'cade', 'remix'],\n",
       " ['sorry', 'not', 'sorry'],\n",
       " ['redbone'],\n",
       " ['24k', 'magic'],\n",
       " ['dna'],\n",
       " ['el', 'amante'],\n",
       " ['you', 'dont', 'know', 'me', 'radio', 'edit'],\n",
       " ['chained', 'to', 'the', 'rhythm'],\n",
       " ['no', 'promises', 'feat', 'demi', 'lovato'],\n",
       " ['dont', 'wan', 'na', 'know', 'feat', 'kendrick', 'lamar'],\n",
       " ['how', 'far', 'ill', 'go', 'from', 'moana'],\n",
       " ['slow', 'hands'],\n",
       " ['escápate', 'conmigo'],\n",
       " ['bounce', 'back'],\n",
       " ['sign', 'of', 'the', 'times'],\n",
       " ['goosebumps'],\n",
       " ['young', 'dumb', 'broke'],\n",
       " ['there', 'for', 'you'],\n",
       " ['cold', 'feat', 'future'],\n",
       " ['silence'],\n",
       " ['too', 'good', 'at', 'goodbyes'],\n",
       " ['just', 'hold', 'on'],\n",
       " ['look', 'what', 'you', 'made', 'me', 'do'],\n",
       " ['glorious', 'feat', 'skylar', 'grey'],\n",
       " ['starving'],\n",
       " ['reggaetón', 'lento', 'bailemos'],\n",
       " ['weak'],\n",
       " ['side', 'to', 'side'],\n",
       " ['otra', 'vez', 'feat', 'j', 'balvin'],\n",
       " ['i', 'like', 'me', 'better'],\n",
       " ['in', 'the', 'name', 'of', 'love'],\n",
       " ['cold', 'water', 'feat', 'justin', 'bieber', 'mø'],\n",
       " ['malibu'],\n",
       " ['all', 'night'],\n",
       " ['hear', 'me', 'now'],\n",
       " ['your', 'song'],\n",
       " ['ahora', 'dice'],\n",
       " ['friends', 'with', 'bloodpop'],\n",
       " ['bank', 'account'],\n",
       " ['bad', 'things', 'with', 'camila', 'cabello'],\n",
       " ['dont', 'let', 'me', 'down'],\n",
       " ['body', 'like', 'a', 'back', 'road'],\n",
       " ['now', 'or', 'never'],\n",
       " ['dusk', 'till', 'dawn', 'radio', 'edit']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# tokenize each title using list comprehension\n",
    "song_titles = songs_df['cleaned_title'].tolist()\n",
    "tokenized = []\n",
    "\n",
    "for song in song_titles:\n",
    "    tokenized.append(word_tokenize(song))\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;6.</font><font color=\"salmon\"> Method 1 - Lemmatisation </font> </h2></a>\n",
    "\n",
    "Lemmatisation is a more accurate but slower version of stemming. It works in two parts:\n",
    "\n",
    "1. Part Of Speech Tagging\n",
    "2. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.1'><h3>6.1 Part-Of-Speech (POS) Tagging</h3></a>\n",
    "\n",
    "POS Tagging basically classifies a word as a \n",
    "\n",
    "* nouns,\n",
    "* verbs,\n",
    "* adjectives,\n",
    "* adverbs,\n",
    "* etc.\n",
    "\n",
    "<img src=\"https://i.imgur.com/GQTUrJk.png\" width=800>\n",
    "\n",
    "It takes a list of tokens and adds a tag to each of them\n",
    "\n",
    "<img src=\"https://i.imgur.com/1hDGlsu.png\" width=\"500\"/>\n",
    "\n",
    "The meaning of these tags can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('consolidating', 'VBG'),\n",
       " ('credit', 'NN'),\n",
       " ('card', 'NN'),\n",
       " ('debt', 'NN'),\n",
       " ('incurred', 'VBN'),\n",
       " ('over', 'IN'),\n",
       " ('three', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('ago', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('having', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('concrete', 'JJ'),\n",
       " ('end', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('sight', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('more', 'RBR'),\n",
       " ('motivating', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('eagerly', 'RB'),\n",
       " ('striving', 'JJ'),\n",
       " ('towards', 'NNS'),\n",
       " ('becoming', 'VBG'),\n",
       " ('completely', 'RB'),\n",
       " ('debt', 'NN'),\n",
       " ('free', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
    "\n",
    "tagged_tokkens = nltk.pos_tag(tokens)\n",
    "tagged_tokkens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6.2'><h3>6.2 Lemmatisation</h3></a>\n",
    "\n",
    "<img src=\"https://i.imgur.com/dcps8NV.png\" width=\"400\"/>\n",
    "\n",
    "Lemmatisation, like stemming converts words to their root form.\n",
    "\n",
    "* **HOWEVER**, unlike stemming it doesn't arbitrarily chop off letters.\n",
    "* Instead, it refers to a vocabulary and converts words into their base form\n",
    "* The tokens need to be tagged (which you just did)\n",
    "\n",
    "```\n",
    "  Raw               Stemming        Lemmatisation\n",
    "  --------------    --------------  --------------\n",
    "  consolidating          consolid       consolidate\n",
    "```\n",
    "\n",
    "From the above example we can see that **consolidating** is converted to **consolidate**\n",
    "\n",
    "#### Lemmatisation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'consolidate'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('consolidating', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lemmatisation as a Function \n",
    "\n",
    "As the lemmatizer works on only a single word, you will have to apply it to each pair to get a lemmatized sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(pair):\n",
    "    word, tag = pair\n",
    "\n",
    "    try:\n",
    "        return lemmatizer.lemmatize(word, pos=tag[0].lower())\n",
    "    except KeyError:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'be',\n",
       " 'consolidate',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'debt',\n",
       " 'incur',\n",
       " 'over',\n",
       " 'three',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'have',\n",
       " 'a',\n",
       " 'concrete',\n",
       " 'end',\n",
       " 'in',\n",
       " 'sight',\n",
       " 'be',\n",
       " 'more',\n",
       " 'motivating',\n",
       " '.',\n",
       " 'I',\n",
       " 'be',\n",
       " 'eagerly',\n",
       " 'striving',\n",
       " 'towards',\n",
       " 'become',\n",
       " 'completely',\n",
       " 'debt',\n",
       " 'free',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
    "# nltk.pos_tag(a list of tokens)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "lemmatized = []\n",
    "\n",
    "for pair in tagged_tokkens:\n",
    "    lemmatized.append(lemmatize(pair))\n",
    "\n",
    "lemmatized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;7.</font><font color=\"salmon\"> Method 2 - Stemming </font> </h2></a>\n",
    "\n",
    "Stemming makes words common by chopping off the ends of a word. \n",
    "\n",
    "A common algorithm is Porter's Algorithm. It will convert all of the following words to **`oper`**\n",
    "\n",
    "<img src=\"https://i.imgur.com/trzAobw.png\" width=\"200\"/>\n",
    "\n",
    "How it works is by applying a set of rules to the word\n",
    "\n",
    "**Step 1a** of the Porter Algorithm\n",
    "```\n",
    "SSES ->  SS        caresses  ->  caress\n",
    "IES  ->  I         ponies  ->  poni\n",
    "SS  ->  SS         caress  ->  caress\n",
    "S  ->              cats  ->  cat\n",
    "```\n",
    "\n",
    "More steps [here](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Create a stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Now you can stem any word you want\n",
    "stemmer.stem(word)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'consolid',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'debt',\n",
       " 'incur',\n",
       " 'over',\n",
       " 'three',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'have',\n",
       " 'a',\n",
       " 'concret',\n",
       " 'end',\n",
       " 'in',\n",
       " 'sight',\n",
       " 'is',\n",
       " 'more',\n",
       " 'motiv',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'eagerli',\n",
       " 'strive',\n",
       " 'toward',\n",
       " 'becom',\n",
       " 'complet',\n",
       " 'debt',\n",
       " 'free',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem each word using list comprehension\n",
    "stemmed = []\n",
    "\n",
    "for token in tokens:\n",
    "    stemmed.append(stemmer.stem(token))\n",
    "\n",
    "stemmed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;8.</font><font color=\"salmon\"> Stemming vs. Lemming </font> </h2></a>\n",
    "\n",
    "<img src=\"https://i.imgur.com/7CgyWwH.png\" width=500>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    <b>Below is a difference between Stemming and Lemmatization</b>\n",
    "\n",
    " ```\n",
    "   Raw               Stemming        Lemmatisation\n",
    "   --------------    --------------  --------------\n",
    "              I                 I                 I\n",
    "             am                am                be\n",
    "  consolidating          consolid       consolidate\n",
    "         credit            credit            credit\n",
    "           card              card              card\n",
    "           debt              debt              debt\n",
    "       incurred             incur             incur\n",
    "           over              over              over\n",
    "          three             three             three\n",
    "          years              year              year\n",
    "            ago               ago               ago\n",
    "            and               and               and\n",
    "         having              have              have\n",
    "              a                 a                 a\n",
    "       concrete           concret          concrete\n",
    "            end               end               end\n",
    "             in                in                in\n",
    "          sight             sight             sight\n",
    "             is                is                be\n",
    "           more              more              more\n",
    "     motivating             motiv        motivating\n",
    "              .                 .                 .\n",
    "              I                 I                 I\n",
    "             am                am                be\n",
    "        eagerly           eagerli           eagerly\n",
    "       striving            strive          striving\n",
    "        towards            toward           towards\n",
    "       becoming             becom            become\n",
    "     completely           complet        completely\n",
    "           debt              debt              debt\n",
    "           free              free              free\n",
    " ```\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;9.</font><font color=\"salmon\"> Stop Word Removal </font> </h2></a>\n",
    "\n",
    "Stop words are words which have little to no meaning on the overall sentence. For example, they are\n",
    "\n",
    "* i\n",
    "* me\n",
    "* my\n",
    "* or\n",
    "* a\n",
    "* an\n",
    "\n",
    "A more complete list can be downloaded from the **`stopwords` corpus**.\n",
    "\n",
    "The **`stopwords`** corpus that you downloaded is basically a list words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'a',\n",
       " 'ad',\n",
       " 'altı',\n",
       " 'altmış',\n",
       " 'amma',\n",
       " 'arasında',\n",
       " 'artıq',\n",
       " 'ay',\n",
       " 'az',\n",
       " 'bax',\n",
       " 'belə',\n",
       " 'bəli',\n",
       " 'bəlkə',\n",
       " 'beş',\n",
       " 'bəy',\n",
       " 'bəzən',\n",
       " 'bəzi',\n",
       " 'bilər',\n",
       " 'bir',\n",
       " 'biraz',\n",
       " 'biri',\n",
       " 'birşey',\n",
       " 'biz',\n",
       " 'bizim',\n",
       " 'bizlər',\n",
       " 'bu',\n",
       " 'buna',\n",
       " 'bundan',\n",
       " 'bunların',\n",
       " 'bunu',\n",
       " 'bunun',\n",
       " 'buradan',\n",
       " 'bütün',\n",
       " 'ci',\n",
       " 'cı',\n",
       " 'çox',\n",
       " 'cu',\n",
       " 'cü',\n",
       " 'çünki',\n",
       " 'da',\n",
       " 'daha',\n",
       " 'də',\n",
       " 'dedi',\n",
       " 'dək',\n",
       " 'dən',\n",
       " 'dəqiqə',\n",
       " 'deyil',\n",
       " 'dir',\n",
       " 'doqquz',\n",
       " 'doqsan',\n",
       " 'dörd',\n",
       " 'düz',\n",
       " 'ə',\n",
       " 'edən',\n",
       " 'edir',\n",
       " 'əgər',\n",
       " 'əlbəttə',\n",
       " 'elə',\n",
       " 'əlli',\n",
       " 'ən',\n",
       " 'əslində',\n",
       " 'et',\n",
       " 'etdi',\n",
       " 'etmə',\n",
       " 'etmək',\n",
       " 'faiz',\n",
       " 'gilə',\n",
       " 'görə',\n",
       " 'ha',\n",
       " 'haqqında',\n",
       " 'harada',\n",
       " 'hə',\n",
       " 'heç',\n",
       " 'həm',\n",
       " 'həmin',\n",
       " 'həmişə',\n",
       " 'hər',\n",
       " 'ı',\n",
       " 'idi',\n",
       " 'iki',\n",
       " 'il',\n",
       " 'ildə',\n",
       " 'ilə',\n",
       " 'ilk',\n",
       " 'in',\n",
       " 'indi',\n",
       " 'isə',\n",
       " 'istifadə',\n",
       " 'iyirmi',\n",
       " 'ki',\n",
       " 'kim',\n",
       " 'kimə',\n",
       " 'kimi',\n",
       " 'lakin',\n",
       " 'lap',\n",
       " 'məhz',\n",
       " 'mən',\n",
       " 'mənə',\n",
       " 'mirşey',\n",
       " 'nə',\n",
       " 'nəhayət',\n",
       " 'niyə',\n",
       " 'o',\n",
       " 'obirisi',\n",
       " 'of',\n",
       " 'olan',\n",
       " 'olar',\n",
       " 'olaraq',\n",
       " 'oldu',\n",
       " 'olduğu',\n",
       " 'olmadı',\n",
       " 'olmaz',\n",
       " 'olmuşdur',\n",
       " 'olsun',\n",
       " 'olur',\n",
       " 'on',\n",
       " 'ona',\n",
       " 'ondan',\n",
       " 'onlar',\n",
       " 'onlardan',\n",
       " 'onların ',\n",
       " 'onsuzda',\n",
       " 'onu',\n",
       " 'onun',\n",
       " 'oradan',\n",
       " 'otuz',\n",
       " 'öz',\n",
       " 'özü',\n",
       " 'qarşı',\n",
       " 'qədər',\n",
       " 'qırx',\n",
       " 'saat',\n",
       " 'sadəcə',\n",
       " 'saniyə',\n",
       " 'səhv',\n",
       " 'səkkiz',\n",
       " 'səksən',\n",
       " 'sən',\n",
       " 'sənə',\n",
       " 'sənin',\n",
       " 'siz',\n",
       " 'sizin',\n",
       " 'sizlər',\n",
       " 'sonra',\n",
       " 'təəssüf',\n",
       " 'ü',\n",
       " 'üç',\n",
       " 'üçün',\n",
       " 'var',\n",
       " 'və',\n",
       " 'xan',\n",
       " 'xanım',\n",
       " 'xeyr',\n",
       " 'ya',\n",
       " 'yalnız',\n",
       " 'yaxşı',\n",
       " 'yeddi',\n",
       " 'yenə',\n",
       " 'yəni',\n",
       " 'yetmiş',\n",
       " 'yox',\n",
       " 'yoxdur',\n",
       " 'yoxsa',\n",
       " 'yüz',\n",
       " 'zamanog',\n",
       " 'i',\n",
       " 'jeg',\n",
       " 'det',\n",
       " 'at',\n",
       " 'en',\n",
       " 'den',\n",
       " 'til',\n",
       " 'er',\n",
       " 'som',\n",
       " 'på',\n",
       " 'de',\n",
       " 'med',\n",
       " 'han',\n",
       " 'af',\n",
       " 'for',\n",
       " 'ikke',\n",
       " 'der',\n",
       " 'var',\n",
       " 'mig',\n",
       " 'sig',\n",
       " 'men',\n",
       " 'et',\n",
       " 'har',\n",
       " 'om',\n",
       " 'vi',\n",
       " 'min',\n",
       " 'havde',\n",
       " 'ham',\n",
       " 'hun',\n",
       " 'nu',\n",
       " 'over',\n",
       " 'da',\n",
       " 'fra',\n",
       " 'du',\n",
       " 'ud',\n",
       " 'sin',\n",
       " 'dem',\n",
       " 'os',\n",
       " 'op',\n",
       " 'man',\n",
       " 'hans',\n",
       " 'hvor',\n",
       " 'eller',\n",
       " 'hvad',\n",
       " 'skal',\n",
       " 'selv',\n",
       " 'her',\n",
       " 'alle',\n",
       " 'vil',\n",
       " 'blev',\n",
       " 'kunne',\n",
       " 'ind',\n",
       " 'når',\n",
       " 'være',\n",
       " 'dog',\n",
       " 'noget',\n",
       " 'ville',\n",
       " 'jo',\n",
       " 'deres',\n",
       " 'efter',\n",
       " 'ned',\n",
       " 'skulle',\n",
       " 'denne',\n",
       " 'end',\n",
       " 'dette',\n",
       " 'mit',\n",
       " 'også',\n",
       " 'under',\n",
       " 'have',\n",
       " 'dig',\n",
       " 'anden',\n",
       " 'hende',\n",
       " 'mine',\n",
       " 'alt',\n",
       " 'meget',\n",
       " 'sit',\n",
       " 'sine',\n",
       " 'vor',\n",
       " 'mod',\n",
       " 'disse',\n",
       " 'hvis',\n",
       " 'din',\n",
       " 'nogle',\n",
       " 'hos',\n",
       " 'blive',\n",
       " 'mange',\n",
       " 'ad',\n",
       " 'bliver',\n",
       " 'hendes',\n",
       " 'været',\n",
       " 'thi',\n",
       " 'jer',\n",
       " 'sådan',\n",
       " 'de',\n",
       " 'en',\n",
       " 'van',\n",
       " 'ik',\n",
       " 'te',\n",
       " 'dat',\n",
       " 'die',\n",
       " 'in',\n",
       " 'een',\n",
       " 'hij',\n",
       " 'het',\n",
       " 'niet',\n",
       " 'zijn',\n",
       " 'is',\n",
       " 'was',\n",
       " 'op',\n",
       " 'aan',\n",
       " 'met',\n",
       " 'als',\n",
       " 'voor',\n",
       " 'had',\n",
       " 'er',\n",
       " 'maar',\n",
       " 'om',\n",
       " 'hem',\n",
       " 'dan',\n",
       " 'zou',\n",
       " 'of',\n",
       " 'wat',\n",
       " 'mijn',\n",
       " 'men',\n",
       " 'dit',\n",
       " 'zo',\n",
       " 'door',\n",
       " 'over',\n",
       " 'ze',\n",
       " 'zich',\n",
       " 'bij',\n",
       " 'ook',\n",
       " 'tot',\n",
       " 'je',\n",
       " 'mij',\n",
       " 'uit',\n",
       " 'der',\n",
       " 'daar',\n",
       " 'haar',\n",
       " 'naar',\n",
       " 'heb',\n",
       " 'hoe',\n",
       " 'heeft',\n",
       " 'hebben',\n",
       " 'deze',\n",
       " 'u',\n",
       " 'want',\n",
       " 'nog',\n",
       " 'zal',\n",
       " 'me',\n",
       " 'zij',\n",
       " 'nu',\n",
       " 'ge',\n",
       " 'geen',\n",
       " 'omdat',\n",
       " 'iets',\n",
       " 'worden',\n",
       " 'toch',\n",
       " 'al',\n",
       " 'waren',\n",
       " 'veel',\n",
       " 'meer',\n",
       " 'doen',\n",
       " 'toen',\n",
       " 'moet',\n",
       " 'ben',\n",
       " 'zonder',\n",
       " 'kan',\n",
       " 'hun',\n",
       " 'dus',\n",
       " 'alles',\n",
       " 'onder',\n",
       " 'ja',\n",
       " 'eens',\n",
       " 'hier',\n",
       " 'wie',\n",
       " 'werd',\n",
       " 'altijd',\n",
       " 'doch',\n",
       " 'wordt',\n",
       " 'wezen',\n",
       " 'kunnen',\n",
       " 'ons',\n",
       " 'zelf',\n",
       " 'tegen',\n",
       " 'na',\n",
       " 'reeds',\n",
       " 'wil',\n",
       " 'kon',\n",
       " 'niets',\n",
       " 'uw',\n",
       " 'iemand',\n",
       " 'geweest',\n",
       " 'andere',\n",
       " 'i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'olla',\n",
       " 'olen',\n",
       " 'olet',\n",
       " 'on',\n",
       " 'olemme',\n",
       " 'olette',\n",
       " 'ovat',\n",
       " 'ole',\n",
       " 'oli',\n",
       " 'olisi',\n",
       " 'olisit',\n",
       " 'olisin',\n",
       " 'olisimme',\n",
       " 'olisitte',\n",
       " 'olisivat',\n",
       " 'olit',\n",
       " 'olin',\n",
       " 'olimme',\n",
       " 'olitte',\n",
       " 'olivat',\n",
       " 'ollut',\n",
       " 'olleet',\n",
       " 'en',\n",
       " 'et',\n",
       " 'ei',\n",
       " 'emme',\n",
       " 'ette',\n",
       " 'eivät',\n",
       " 'minä',\n",
       " 'minun',\n",
       " 'minut',\n",
       " 'minua',\n",
       " 'minussa',\n",
       " 'minusta',\n",
       " 'minuun',\n",
       " 'minulla',\n",
       " 'minulta',\n",
       " 'minulle',\n",
       " 'sinä',\n",
       " 'sinun',\n",
       " 'sinut',\n",
       " 'sinua',\n",
       " 'sinussa',\n",
       " 'sinusta',\n",
       " 'sinuun',\n",
       " 'sinulla',\n",
       " 'sinulta',\n",
       " 'sinulle',\n",
       " 'hän',\n",
       " 'hänen',\n",
       " 'hänet',\n",
       " 'häntä',\n",
       " 'hänessä',\n",
       " 'hänestä',\n",
       " 'häneen',\n",
       " 'hänellä',\n",
       " 'häneltä',\n",
       " 'hänelle',\n",
       " 'me',\n",
       " 'meidän',\n",
       " 'meidät',\n",
       " 'meitä',\n",
       " 'meissä',\n",
       " 'meistä',\n",
       " 'meihin',\n",
       " 'meillä',\n",
       " 'meiltä',\n",
       " 'meille',\n",
       " 'te',\n",
       " 'teidän',\n",
       " 'teidät',\n",
       " 'teitä',\n",
       " 'teissä',\n",
       " 'teistä',\n",
       " 'teihin',\n",
       " 'teillä',\n",
       " 'teiltä',\n",
       " 'teille',\n",
       " 'he',\n",
       " 'heidän',\n",
       " 'heidät',\n",
       " 'heitä',\n",
       " 'heissä',\n",
       " 'heistä',\n",
       " 'heihin',\n",
       " 'heillä',\n",
       " 'heiltä',\n",
       " 'heille',\n",
       " 'tämä',\n",
       " 'tämän',\n",
       " 'tätä',\n",
       " 'tässä',\n",
       " 'tästä',\n",
       " 'tähän',\n",
       " 'tallä',\n",
       " 'tältä',\n",
       " 'tälle',\n",
       " 'tänä',\n",
       " 'täksi',\n",
       " 'tuo',\n",
       " 'tuon',\n",
       " 'tuotä',\n",
       " 'tuossa',\n",
       " 'tuosta',\n",
       " 'tuohon',\n",
       " 'tuolla',\n",
       " 'tuolta',\n",
       " 'tuolle',\n",
       " 'tuona',\n",
       " 'tuoksi',\n",
       " 'se',\n",
       " 'sen',\n",
       " 'sitä',\n",
       " 'siinä',\n",
       " 'siitä',\n",
       " 'siihen',\n",
       " 'sillä',\n",
       " 'siltä',\n",
       " 'sille',\n",
       " 'sinä',\n",
       " 'siksi',\n",
       " 'nämä',\n",
       " 'näiden',\n",
       " 'näitä',\n",
       " 'näissä',\n",
       " 'näistä',\n",
       " 'näihin',\n",
       " 'näillä',\n",
       " 'näiltä',\n",
       " 'näille',\n",
       " 'näinä',\n",
       " 'näiksi',\n",
       " 'nuo',\n",
       " 'noiden',\n",
       " 'noita',\n",
       " 'noissa',\n",
       " 'noista',\n",
       " 'noihin',\n",
       " 'noilla',\n",
       " 'noilta',\n",
       " 'noille',\n",
       " 'noina',\n",
       " 'noiksi',\n",
       " 'ne',\n",
       " 'niiden',\n",
       " 'niitä',\n",
       " 'niissä',\n",
       " 'niistä',\n",
       " 'niihin',\n",
       " 'niillä',\n",
       " 'niiltä',\n",
       " 'niille',\n",
       " 'niinä',\n",
       " 'niiksi',\n",
       " 'kuka',\n",
       " 'kenen',\n",
       " 'kenet',\n",
       " 'ketä',\n",
       " 'kenessä',\n",
       " 'kenestä',\n",
       " 'keneen',\n",
       " 'kenellä',\n",
       " 'keneltä',\n",
       " 'kenelle',\n",
       " 'kenenä',\n",
       " 'keneksi',\n",
       " 'ketkä',\n",
       " 'keiden',\n",
       " 'ketkä',\n",
       " 'keitä',\n",
       " 'keissä',\n",
       " 'keistä',\n",
       " 'keihin',\n",
       " 'keillä',\n",
       " 'keiltä',\n",
       " 'keille',\n",
       " 'keinä',\n",
       " 'keiksi',\n",
       " 'mikä',\n",
       " 'minkä',\n",
       " 'minkä',\n",
       " 'mitä',\n",
       " 'missä',\n",
       " 'mistä',\n",
       " 'mihin',\n",
       " 'millä',\n",
       " 'miltä',\n",
       " 'mille',\n",
       " 'minä',\n",
       " 'miksi',\n",
       " 'mitkä',\n",
       " 'joka',\n",
       " 'jonka',\n",
       " 'jota',\n",
       " 'jossa',\n",
       " 'josta',\n",
       " 'johon',\n",
       " 'jolla',\n",
       " 'jolta',\n",
       " 'jolle',\n",
       " 'jona',\n",
       " 'joksi',\n",
       " 'jotka',\n",
       " 'joiden',\n",
       " 'joita',\n",
       " 'joissa',\n",
       " 'joista',\n",
       " 'joihin',\n",
       " 'joilla',\n",
       " 'joilta',\n",
       " 'joille',\n",
       " 'joina',\n",
       " 'joiksi',\n",
       " 'että',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOP_WORDS = stopwords.words()\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To remove the stop words, you just filter out tokens which are in the list of stop words\n",
    "\n",
    "* For a string `text` and a list `l`, you can check whether `l` contains `text` by doing\n",
    "\n",
    "```python\n",
    "\n",
    "    # Example\n",
    "    text = 'a'\n",
    "    ab_list = ['a','b']\n",
    "\n",
    "    text in ab_list # True\n",
    "    'c' in ab_list # False\n",
    "```\n",
    "\n",
    "* Here's how an example of how you can choose only tokens that don't exist in stopwords\n",
    "\n",
    "\n",
    "```python\n",
    "tokens = ['Hi', 'I']\n",
    "\n",
    "no_stop_words = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word not in STOP_WORDS:\n",
    "            no_stop_words.append(word)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['consolidating',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'debt',\n",
       " 'incurred',\n",
       " 'three',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'concrete',\n",
       " 'sight',\n",
       " 'motivating',\n",
       " '.',\n",
       " 'eagerly',\n",
       " 'striving',\n",
       " 'towards',\n",
       " 'becoming',\n",
       " 'completely',\n",
       " 'debt',\n",
       " 'free',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens =['I', 'am', 'consolidating', 'credit', 'card', 'debt', 'incurred', 'over', 'three', 'years', 'ago', 'and', 'having', 'a', 'concrete', 'end', 'in', 'sight', 'is', 'more', 'motivating', '.', 'I', 'am', 'eagerly', 'striving', 'towards', 'becoming', 'completely', 'debt', 'free', '.']\n",
    "\n",
    "# Write code below\n",
    "filtered = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word.lower() not in STOP_WORDS:\n",
    "        filtered.append(word.lower())\n",
    "        \n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='P2'><h2> <img src=\"https://cdn.shopify.com/s/files/1/1200/7374/products/book_aec28e76-52ec-44ab-bc01-41df1279c89f_550x825.png?v=1473897430\" width=25 align=\"left\"> <font color=\"darkorange\"> &nbsp; Practice II </font><font color=\"skyblue\"> * </font></h2></a>\n",
    "\n",
    "## Loans Description \n",
    "\n",
    "Open the `loans-descs-1k.csv` file as a DataFrame called `loans_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>grade</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296</td>\n",
       "      <td>7337222</td>\n",
       "      <td>8999285</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>D</td>\n",
       "      <td>Borrower added on 09/17/13 &gt; The wedding of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>844</td>\n",
       "      <td>7365470</td>\n",
       "      <td>9027579</td>\n",
       "      <td>31825.0</td>\n",
       "      <td>C</td>\n",
       "      <td>Borrower added on 09/15/13 &gt; Pay Off High Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110</td>\n",
       "      <td>676471</td>\n",
       "      <td>864466</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>G</td>\n",
       "      <td>Borrower added on 02/15/11 &gt; My husband and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>278</td>\n",
       "      <td>612322</td>\n",
       "      <td>785185</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Borrower added on 11/09/10 &gt; debt refi, alwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>951</td>\n",
       "      <td>7051350</td>\n",
       "      <td>8713086</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>Borrower added on 02/14/14 &gt; I am consolidat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id  member_id  loan_amnt grade  \\\n",
       "0         296  7337222    8999285    10000.0     D   \n",
       "1         844  7365470    9027579    31825.0     C   \n",
       "2         110   676471     864466    20000.0     G   \n",
       "3         278   612322     785185    20000.0     F   \n",
       "4         951  7051350    8713086    13000.0     B   \n",
       "\n",
       "                                                desc  \n",
       "0    Borrower added on 09/17/13 > The wedding of ...  \n",
       "1    Borrower added on 09/15/13 > Pay Off High Cr...  \n",
       "2    Borrower added on 02/15/11 > My husband and ...  \n",
       "3    Borrower added on 11/09/10 > debt refi, alwa...  \n",
       "4    Borrower added on 02/14/14 > I am consolidat...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "loans_df = pd.read_csv(\"loans-descs-1k.csv\")\n",
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cleaning function\n",
    "\n",
    "Create a cleaning function called `clean()` that cleans the text by applying the following transformations to the `desc` column\n",
    "\n",
    "* 3 regexes with the patterns (do it 1 at a time)\n",
    "\n",
    "```\n",
    "Borrower added on \\d+/\\d+/\\d+ >|<br>\n",
    "<[a-z]+/?>\n",
    "[-_,$&!.;%]\n",
    "```\n",
    "\n",
    "* Strip any extra spaces\n",
    "* Lowercases the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Clean loans table here\n",
    "def clean(description):\n",
    "    description = re.sub(\"Borrower added on \\d+/\\d+/\\d+ >|<br>\", \"\", description)\n",
    "    description = re.sub(\"<[a-z]+/?>\", \"\", description)\n",
    "    description = re.sub(\"[-_,$&!.;%]\", \"\", description)\n",
    "    description = description.strip()\n",
    "    description = description.lower()\n",
    "    return description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the cleaning function\n",
    "\n",
    "Using the `clean()` function, apply it to the `desc` column of the DataFrame, then <b>reassigning it back to the same column.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>grade</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296</td>\n",
       "      <td>7337222</td>\n",
       "      <td>8999285</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>D</td>\n",
       "      <td>the wedding of our dreams  my fianceacute and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>844</td>\n",
       "      <td>7365470</td>\n",
       "      <td>9027579</td>\n",
       "      <td>31825.0</td>\n",
       "      <td>C</td>\n",
       "      <td>pay off high credit card balances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110</td>\n",
       "      <td>676471</td>\n",
       "      <td>864466</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>G</td>\n",
       "      <td>my husband and i have just purchased a new hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>278</td>\n",
       "      <td>612322</td>\n",
       "      <td>785185</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>F</td>\n",
       "      <td>debt refi always pay bills on time  debt refi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>951</td>\n",
       "      <td>7051350</td>\n",
       "      <td>8713086</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>B</td>\n",
       "      <td>i am consolidating credit card debt incurred o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id  member_id  loan_amnt grade  \\\n",
       "0         296  7337222    8999285    10000.0     D   \n",
       "1         844  7365470    9027579    31825.0     C   \n",
       "2         110   676471     864466    20000.0     G   \n",
       "3         278   612322     785185    20000.0     F   \n",
       "4         951  7051350    8713086    13000.0     B   \n",
       "\n",
       "                                                desc  \n",
       "0  the wedding of our dreams  my fianceacute and ...  \n",
       "1                  pay off high credit card balances  \n",
       "2  my husband and i have just purchased a new hom...  \n",
       "3      debt refi always pay bills on time  debt refi  \n",
       "4  i am consolidating credit card debt incurred o...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_df['desc'] = loans_df['desc'].apply(clean)\n",
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise 1 Description\n",
    "\n",
    "Randomly select the **first** description and apply:\n",
    "\n",
    "1. Tokenization\n",
    "2. Stemming\n",
    "3. Stopword Removal\n",
    "\n",
    "**Expected output:**\n",
    "\n",
    "    ['wed', 'dream', 'fianceacut', 'plan', 'familyfocus', 'wed', 'hometown', 'us', 'larg', 'famili', 'parent', 'retir', \"'re\", 'cover', 'expens', 'wed', 'prove', 'expens', 'anticip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pay', 'off', 'some', 'debt', 'i', 'have', 'incur']\n",
      "['pay', 'debt', 'incur']\n"
     ]
    }
   ],
   "source": [
    "desc = loans_df['desc'][10]\n",
    "\n",
    "desc_tokens = word_tokenize(desc)\n",
    "stemmed = []\n",
    "\n",
    "for word in desc_tokens:\n",
    "    stemmed.append(stemmer.stem(word))\n",
    "    \n",
    "print(stemmed)\n",
    "\n",
    "stemmed_no_stopwords = []\n",
    "for word in stemmed:\n",
    "    if word.lower() not in STOP_WORDS:\n",
    "        stemmed_no_stopwords.append(word.lower())\n",
    "        \n",
    "print(stemmed_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3> End of Practice II </h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'><h2><img src=\"https://images.vexels.com/media/users/3/153978/isolated/preview/483ef8b10a46e28d02293a31570c8c56-warning-sign-colored-stroke-icon-by-vexels.png\" width=23 align=\"left\"><font color=\"salmon\">&nbsp;10.</font><font color=\"salmon\"> Sentiment Analysis </font> </h2></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10.1'><h3>10.1 VADER</h3></a>\n",
    "\n",
    "The `nltk` library has a sentiment analyser. It uses the VADER method or **Valence Aware Dictionary for\n",
    "sEntiment Reasoning**. It is a lexicon (vocabulary) of words and their relative sentiment strength. For example:\n",
    "    \n",
    "- `Good` has a positive but weak score, while `Excellent` scores more\n",
    "- `Bad` has a negative but weaks score, while `Tragedy` scores more\n",
    "\n",
    "Use `sid.polarity_scores(t)` to find the sentiment of a text. \n",
    "\n",
    "Use the `compound` value to determine the overall score. Note that `compound` give a (normalised) value from $-1$ to $1$, and hence a positive number is good sentiment while a negative number is bad sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\michael\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the sentiment scores change based on the sentiment of a movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.754, 'pos': 0.246, 'compound': 0.5563}\n",
      "0.5563\n"
     ]
    }
   ],
   "source": [
    "review_1 = \"\"\"I thoroughly enjoyed this movie because there was a genuine sincerity in the acting.\"\"\"\n",
    "ss = sid.polarity_scores(review_1)\n",
    "print(ss)\n",
    "print(ss['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it yourself with the following 2 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.326, 'neu': 0.503, 'pos': 0.171, 'compound': -0.3025}\n"
     ]
    }
   ],
   "source": [
    "review_2 = \"I found it really boring and silly.\"\n",
    "\n",
    "ss = sid.polarity_scores(review_2)\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.381, 'neu': 0.309, 'pos': 0.309, 'compound': -0.1779}\n"
     ]
    }
   ],
   "source": [
    "review_3 = \"My personal favorite horror film.\"\n",
    "\n",
    "ss = sid.polarity_scores(review_3)\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10.2'><h3>10.2 Naive Bayes Classifier</h3></a>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Naive Bayes Classifier will be covered in the next lesson on Machine Learning. \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
